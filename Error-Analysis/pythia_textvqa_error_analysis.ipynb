{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pythia_textvqa_error_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXeEePmeEuuY",
        "colab_type": "text"
      },
      "source": [
        "## First download all of the necessary data\n",
        "\n",
        "---\n",
        "\n",
        "Press \"Shift + Enter\" to run each cell sequentially. Alternatively, you can press \"Cmd/Ctrl + F9\" to run all cells and then scroll down to bottom cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRUp0r_-B9N0",
        "colab_type": "code",
        "outputId": "fbc44c48-49ac-4981-c627-dd3edd0b3445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Download Pre-requisites needed for running the e2e model\n",
        "%cd /content/\n",
        "\n",
        "%mkdir model_data\n",
        "!wget -O /content/model_data/answers_vqa.txt https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\n",
        "!wget -O /content/model_data/vocabulary_100k.txt https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt\n",
        "!wget -O /content/model_data/detectron_model.pth  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth \n",
        "!wget -O /content/model_data/pythia.pth https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth\n",
        "!wget -O /content/model_data/pythia.yaml https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.yml\n",
        "!wget -O /content/model_data/detectron_model.yaml https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n",
        "!wget -O /content/model_data/detectron_weights.tar.gz https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz\n",
        "!tar xf /content/model_data/detectron_weights.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘model_data’: File exists\n",
            "--2020-04-19 01:07:24--  https://dl.fbaipublicfiles.com/pythia/data/answers_vqa.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24768 (24K) [text/plain]\n",
            "Saving to: ‘/content/model_data/answers_vqa.txt’\n",
            "\n",
            "/content/model_data 100%[===================>]  24.19K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-04-19 01:07:24 (1.43 MB/s) - ‘/content/model_data/answers_vqa.txt’ saved [24768/24768]\n",
            "\n",
            "--2020-04-19 01:07:25--  https://dl.fbaipublicfiles.com/pythia/data/vocabulary_100k.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 626738 (612K) [text/plain]\n",
            "Saving to: ‘/content/model_data/vocabulary_100k.txt’\n",
            "\n",
            "/content/model_data 100%[===================>] 612.05K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-04-19 01:07:25 (5.25 MB/s) - ‘/content/model_data/vocabulary_100k.txt’ saved [626738/626738]\n",
            "\n",
            "--2020-04-19 01:07:26--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 684079216 (652M) [application/octet-stream]\n",
            "Saving to: ‘/content/model_data/detectron_model.pth’\n",
            "\n",
            "/content/model_data 100%[===================>] 652.39M  22.3MB/s    in 24s     \n",
            "\n",
            "2020-04-19 01:07:50 (27.1 MB/s) - ‘/content/model_data/detectron_model.pth’ saved [684079216/684079216]\n",
            "\n",
            "--2020-04-19 01:07:51--  https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 713440524 (680M) [application/octet-stream]\n",
            "Saving to: ‘/content/model_data/pythia.pth’\n",
            "\n",
            "/content/model_data 100%[===================>] 680.39M  59.3MB/s    in 12s     \n",
            "\n",
            "2020-04-19 01:08:04 (54.8 MB/s) - ‘/content/model_data/pythia.pth’ saved [713440524/713440524]\n",
            "\n",
            "--2020-04-19 01:08:04--  https://dl.fbaipublicfiles.com/pythia/pretrained_models/vqa2/pythia_train_val.yml\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6577 (6.4K) [text/plain]\n",
            "Saving to: ‘/content/model_data/pythia.yaml’\n",
            "\n",
            "/content/model_data 100%[===================>]   6.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-19 01:08:04 (69.0 MB/s) - ‘/content/model_data/pythia.yaml’ saved [6577/6577]\n",
            "\n",
            "--2020-04-19 01:08:05--  https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 918 [text/plain]\n",
            "Saving to: ‘/content/model_data/detectron_model.yaml’\n",
            "\n",
            "/content/model_data 100%[===================>]     918  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-19 01:08:05 (22.6 MB/s) - ‘/content/model_data/detectron_model.yaml’ saved [918/918]\n",
            "\n",
            "--2020-04-19 01:08:06--  https://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31091524 (30M) [application/gzip]\n",
            "Saving to: ‘/content/model_data/detectron_weights.tar.gz’\n",
            "\n",
            "/content/model_data 100%[===================>]  29.65M  40.4MB/s    in 0.7s    \n",
            "\n",
            "2020-04-19 01:08:07 (40.4 MB/s) - ‘/content/model_data/detectron_weights.tar.gz’ saved [31091524/31091524]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4S01cUqE3WJ",
        "colab_type": "text"
      },
      "source": [
        "## Now, install some particular dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQCyXjYyFQzp",
        "colab_type": "code",
        "outputId": "63037d54-99b9-4a20-9448-a158fe6daf86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "# Install dependencies\n",
        "!pip install ninja yacs cython matplotlib demjson\n",
        "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.16)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: demjson in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs) (3.13)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-jshvm3ig\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-jshvm3ig\n",
            "Requirement already satisfied (use --upgrade to upgrade): pycocotools==2.0 from git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (46.1.3)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.16)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.12.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=275292 sha256=8ac9815b5413276eec20fe9fda8223c67a41cf8b621f3691685f957e0c566a9b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cp4_kjsw/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNW1WWWyFkB5",
        "colab_type": "text"
      },
      "source": [
        "## Install fastText for installing Pythia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke7OQtzeFV7z",
        "colab_type": "code",
        "outputId": "948a187b-be64-4ed0-a7a7-3985a63fa1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf fastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git fastText\n",
        "%cd /content/fastText\n",
        "!pip install -e ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 3768 (delta 35), reused 46 (delta 15), pack-reused 3684\u001b[K\n",
            "Receiving objects: 100% (3768/3768), 8.20 MiB | 13.33 MiB/s, done.\n",
            "Resolving deltas: 100% (2354/2354), done.\n",
            "/content/fastText\n",
            "Obtaining file:///content/fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (1.18.2)\n",
            "Installing collected packages: fasttext\n",
            "  Found existing installation: fasttext 0.9.1\n",
            "    Can't uninstall 'fasttext'. No files were found to uninstall.\n",
            "  Running setup.py develop for fasttext\n",
            "Successfully installed fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivSdn9BFFpxp",
        "colab_type": "text"
      },
      "source": [
        "## Install Pythia now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOHchoDW7yqa",
        "colab_type": "code",
        "outputId": "fa2f7cb9-0c8e-43da-885d-b17ae7fa56ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        }
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf pythia\n",
        "!git clone https://github.com/facebookresearch/pythia.git pythia\n",
        "%cd /content/pythia\n",
        "# Don't modify torch version\n",
        "!sed -i '/torch/d' requirements.txt\n",
        "!pip install -e .\n",
        "import sys\n",
        "sys.path.append('/content/pythia')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'pythia'...\n",
            "remote: Enumerating objects: 3808, done.\u001b[K\n",
            "remote: Total 3808 (delta 0), reused 0 (delta 0), pack-reused 3808\u001b[K\n",
            "Receiving objects: 100% (3808/3808), 6.64 MiB | 16.08 MiB/s, done.\n",
            "Resolving deltas: 100% (2469/2469), done.\n",
            "/content/pythia\n",
            "Obtaining file:///content/pythia\n",
            "\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/req/req_install.py\", line 441, in check_if_exists\n",
            "    self.satisfied_by = pkg_resources.get_distribution(str(no_marker))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 481, in get_distribution\n",
            "    dist = get_provider(dist)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 357, in get_provider\n",
            "    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 900, in require\n",
            "    needed = self.resolve(parse_requirements(requirements))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 791, in resolve\n",
            "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
            "pip._vendor.pkg_resources.ContextualVersionConflict: (tqdm 4.19.9 (/usr/local/lib/python3.6/dist-packages), Requirement.parse('tqdm>=4.27'), {'transformers'})\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 382, in run\n",
            "    resolver.resolve(requirement_set)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 201, in resolve\n",
            "    self._resolve_one(requirement_set, req)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 365, in _resolve_one\n",
            "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/legacy_resolve.py\", line 295, in _get_abstract_dist_for\n",
            "    req, self.require_hashes, self.use_user_site, self.finder,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/prepare.py\", line 263, in prepare_editable_requirement\n",
            "    req.check_if_exists(use_user_site)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/req/req_install.py\", line 452, in check_if_exists\n",
            "    self.req.name\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 481, in get_distribution\n",
            "    dist = get_provider(dist)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 357, in get_provider\n",
            "    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 900, in require\n",
            "    needed = self.resolve(parse_requirements(requirements))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 791, in resolve\n",
            "    raise VersionConflict(dist, req).with_context(dependent_req)\n",
            "pip._vendor.pkg_resources.ContextualVersionConflict: (tqdm 4.19.9 (/usr/local/lib/python3.6/dist-packages), Requirement.parse('tqdm>=4.27'), {'transformers'})\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5o-zqvuFxeR",
        "colab_type": "text"
      },
      "source": [
        "## Install maskrcnn-benchmark now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1ROyH7yG11V",
        "colab_type": "code",
        "outputId": "c1e2cf18-46cc-4cc3-955e-a2766691f736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "# Install maskrcnn-benchmark to extract detectron features\n",
        "%cd /content\n",
        "!git clone https://gitlab.com/meetshah1995/vqa-maskrcnn-benchmark.git\n",
        "%cd /content/vqa-maskrcnn-benchmark\n",
        "# Compile custom layers and build mask-rcnn backbone\n",
        "!python setup.py build\n",
        "!python setup.py develop\n",
        "sys.path.append('/content/vqa-maskrcnn-benchmark')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'vqa-maskrcnn-benchmark' already exists and is not an empty directory.\n",
            "/content/vqa-maskrcnn-benchmark\n",
            "running build\n",
            "running build_py\n",
            "running build_ext\n",
            "running develop\n",
            "running egg_info\n",
            "writing maskrcnn_benchmark.egg-info/PKG-INFO\n",
            "writing dependency_links to maskrcnn_benchmark.egg-info/dependency_links.txt\n",
            "writing top-level names to maskrcnn_benchmark.egg-info/top_level.txt\n",
            "writing manifest file 'maskrcnn_benchmark.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.6/maskrcnn_benchmark/_C.cpython-36m-x86_64-linux-gnu.so -> maskrcnn_benchmark\n",
            "Creating /usr/local/lib/python3.6/dist-packages/maskrcnn-benchmark.egg-link (link to .)\n",
            "maskrcnn-benchmark 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /content/vqa-maskrcnn-benchmark\n",
            "Processing dependencies for maskrcnn-benchmark==0.1\n",
            "Finished processing dependencies for maskrcnn-benchmark==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yx6FeDEF2sw",
        "colab_type": "text"
      },
      "source": [
        "## Demo\n",
        "\n",
        "The class handles everything from feature extraction, token extraction and predicting the answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCD0nso8YelA",
        "colab_type": "code",
        "outputId": "ff196de6-ab12-4bd9-f262-91fed126412e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/\n",
        "import yaml\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from ipywidgets import widgets, Layout\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "from maskrcnn_benchmark.config import cfg\n",
        "from maskrcnn_benchmark.layers import nms\n",
        "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
        "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
        "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
        "\n",
        "\n",
        "from pythia.utils.configuration import ConfigNode\n",
        "from pythia.tasks.processors import VocabProcessor, VQAAnswerProcessor\n",
        "from pythia.models.pythia import Pythia\n",
        "from pythia.common.registry import registry\n",
        "from pythia.common.sample import Sample, SampleList\n",
        "\n",
        "\n",
        "class PythiaDemo:\n",
        "  TARGET_IMAGE_SIZE = [448, 448]\n",
        "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
        "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
        "  \n",
        "  def __init__(self):\n",
        "    self._init_processors()\n",
        "    self.pythia_model = self._build_pythia_model()\n",
        "    self.detection_model = self._build_detection_model()\n",
        "    self.resnet_model = self._build_resnet_model()\n",
        "    \n",
        "  def _init_processors(self):\n",
        "    with open(\"/content/model_data/pythia.yaml\") as f:\n",
        "      config = yaml.load(f)\n",
        "    \n",
        "    config = ConfigNode(config)\n",
        "    # Remove warning\n",
        "    config.training_parameters.evalai_inference = True\n",
        "    registry.register(\"config\", config)\n",
        "    \n",
        "    self.config = config\n",
        "    \n",
        "    vqa_config = config.task_attributes.vqa.dataset_attributes.vqa2\n",
        "    text_processor_config = vqa_config.processors.text_processor\n",
        "    answer_processor_config = vqa_config.processors.answer_processor\n",
        "    \n",
        "    text_processor_config.params.vocab.vocab_file = \"/content/model_data/vocabulary_100k.txt\"\n",
        "    answer_processor_config.params.vocab_file = \"/content/model_data/answers_vqa.txt\"\n",
        "    # Add preprocessor as that will needed when we are getting questions from user\n",
        "    self.text_processor = VocabProcessor(text_processor_config.params)\n",
        "    self.answer_processor = VQAAnswerProcessor(answer_processor_config.params)\n",
        "\n",
        "    registry.register(\"vqa2_text_processor\", self.text_processor)\n",
        "    registry.register(\"vqa2_answer_processor\", self.answer_processor)\n",
        "    registry.register(\"vqa2_num_final_outputs\", \n",
        "                      self.answer_processor.get_vocab_size())\n",
        "    \n",
        "  def _build_pythia_model(self):\n",
        "    state_dict = torch.load('/content/model_data/pythia.pth')\n",
        "    model_config = self.config.model_attributes.pythia\n",
        "    model_config.model_data_dir = \"/content/\"\n",
        "    model = Pythia(model_config)\n",
        "    model.build()\n",
        "    model.init_losses_and_metrics()\n",
        "    \n",
        "    if list(state_dict.keys())[0].startswith('module') and \\\n",
        "       not hasattr(model, 'module'):\n",
        "      state_dict = self._multi_gpu_state_to_single(state_dict)\n",
        "          \n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(\"cuda\")\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "  \n",
        "  def _build_resnet_model(self):\n",
        "    self.data_transforms = transforms.Compose([\n",
        "        transforms.Resize(self.TARGET_IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n",
        "    ])\n",
        "    resnet152 = models.resnet152(pretrained=True)\n",
        "    resnet152.eval()\n",
        "    modules = list(resnet152.children())[:-2]\n",
        "    self.resnet152_model = torch.nn.Sequential(*modules)\n",
        "    self.resnet152_model.to(\"cuda\")\n",
        "  \n",
        "  def _multi_gpu_state_to_single(self, state_dict):\n",
        "    new_sd = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if not k.startswith('module.'):\n",
        "            raise TypeError(\"Not a multiple GPU state of dict\")\n",
        "        k1 = k[7:]\n",
        "        new_sd[k1] = v\n",
        "    return new_sd\n",
        "  \n",
        "  def predict(self, url, question):\n",
        "    with torch.no_grad():\n",
        "      detectron_features = self.get_detectron_features(url)\n",
        "      resnet_features = self.get_resnet_features(url)\n",
        "\n",
        "      sample = Sample()\n",
        "\n",
        "      processed_text = self.text_processor({\"text\": question})\n",
        "      sample.text = processed_text[\"text\"]\n",
        "      sample.text_len = len(processed_text[\"tokens\"])\n",
        "\n",
        "      sample.image_feature_0 = detectron_features\n",
        "      sample.image_info_0 = Sample({\n",
        "          \"max_features\": torch.tensor(100, dtype=torch.long)\n",
        "      })\n",
        "\n",
        "      sample.image_feature_1 = resnet_features\n",
        "\n",
        "      sample_list = SampleList([sample])\n",
        "      sample_list = sample_list.to(\"cuda\")\n",
        "\n",
        "      scores = self.pythia_model(sample_list)[\"scores\"]\n",
        "      scores = torch.nn.functional.softmax(scores, dim=1)\n",
        "      actual, indices = scores.topk(5, dim=1)\n",
        "\n",
        "      top_indices = indices[0]\n",
        "      top_scores = actual[0]\n",
        "\n",
        "      probs = []\n",
        "      answers = []\n",
        "\n",
        "      for idx, score in enumerate(top_scores):\n",
        "        probs.append(score.item())\n",
        "        answers.append(\n",
        "            self.answer_processor.idx2word(top_indices[idx].item())\n",
        "        )\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return probs, answers\n",
        "    \n",
        "  \n",
        "  def _build_detection_model(self):\n",
        "\n",
        "      cfg.merge_from_file('/content/model_data/detectron_model.yaml')\n",
        "      cfg.freeze()\n",
        "\n",
        "      model = build_detection_model(cfg)\n",
        "      checkpoint = torch.load('/content/model_data/detectron_model.pth', \n",
        "                              map_location=torch.device(\"cpu\"))\n",
        "\n",
        "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
        "\n",
        "      model.to(\"cuda\")\n",
        "      model.eval()\n",
        "      return model\n",
        "  \n",
        "  def get_actual_image(self, image_path):\n",
        "      if image_path.startswith('http'):\n",
        "          path = requests.get(image_path, stream=True).raw\n",
        "      else:\n",
        "          path = image_path\n",
        "      \n",
        "      return path\n",
        "\n",
        "  def _image_transform(self, image_path):\n",
        "      path = self.get_actual_image(image_path)\n",
        "\n",
        "      img = Image.open(path)\n",
        "      im = np.array(img).astype(np.float32)\n",
        "      im = im[:, :, ::-1]\n",
        "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
        "      im_shape = im.shape\n",
        "      im_size_min = np.min(im_shape[0:2])\n",
        "      im_size_max = np.max(im_shape[0:2])\n",
        "      im_scale = float(800) / float(im_size_min)\n",
        "      # Prevent the biggest axis from being more than max_size\n",
        "      if np.round(im_scale * im_size_max) > 1333:\n",
        "           im_scale = float(1333) / float(im_size_max)\n",
        "      im = cv2.resize(\n",
        "           im,\n",
        "           None,\n",
        "           None,\n",
        "           fx=im_scale,\n",
        "           fy=im_scale,\n",
        "           interpolation=cv2.INTER_LINEAR\n",
        "       )\n",
        "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
        "      return img, im_scale\n",
        "\n",
        "\n",
        "  def _process_feature_extraction(self, output,\n",
        "                                 im_scales,\n",
        "                                 feat_name='fc6',\n",
        "                                 conf_thresh=0.2):\n",
        "      batch_size = len(output[0][\"proposals\"])\n",
        "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
        "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
        "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
        "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
        "      cur_device = score_list[0].device\n",
        "\n",
        "      feat_list = []\n",
        "\n",
        "      for i in range(batch_size):\n",
        "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
        "          scores = score_list[i]\n",
        "\n",
        "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
        "\n",
        "          for cls_ind in range(1, scores.shape[1]):\n",
        "              cls_scores = scores[:, cls_ind]\n",
        "              keep = nms(dets, cls_scores, 0.5)\n",
        "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
        "                                           cls_scores[keep],\n",
        "                                           max_conf[keep])\n",
        "\n",
        "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
        "          feat_list.append(feats[i][keep_boxes])\n",
        "      return feat_list\n",
        "\n",
        "  def masked_unk_softmax(self, x, dim, mask_idx):\n",
        "      x1 = F.softmax(x, dim=dim)\n",
        "      x1[:, mask_idx] = 0\n",
        "      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
        "      y = x1 / x1_sum\n",
        "      return y\n",
        "   \n",
        "  def get_resnet_features(self, image_path):\n",
        "      path = self.get_actual_image(image_path)\n",
        "      img = Image.open(path).convert(\"RGB\")\n",
        "      img_transform = self.data_transforms(img)\n",
        "      \n",
        "      if img_transform.shape[0] == 1:\n",
        "        img_transform = img_transform.expand(3, -1, -1)\n",
        "      img_transform = img_transform.unsqueeze(0).to(\"cuda\")\n",
        "      \n",
        "      features = self.resnet152_model(img_transform).permute(0, 2, 3, 1)\n",
        "      features = features.view(196, 2048)\n",
        "      return features\n",
        "    \n",
        "  def get_detectron_features(self, image_path):\n",
        "      im, im_scale = self._image_transform(image_path)\n",
        "      img_tensor, im_scales = [im], [im_scale]\n",
        "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
        "      current_img_list = current_img_list.to('cuda')\n",
        "      with torch.no_grad():\n",
        "          output = self.detection_model(current_img_list)\n",
        "      feat_list = self._process_feature_extraction(output, im_scales, \n",
        "                                                  'fc6', 0.2)\n",
        "      return feat_list[0]\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IanIVPt91G",
        "colab_type": "text"
      },
      "source": [
        "### If the command below fails with 'CUDNN_EXECUTION_FAILED', try rerunning the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwF36OmQ72ir",
        "colab_type": "code",
        "outputId": "04dda9e1-70da-4649-9fd6-3cd86e05399a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "demo = PythiaDemo()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pythia/pythia/.vector_cache/glove.6B.zip: 862MB [06:25, 2.23MB/s]                          \n",
            "100%|█████████▉| 399568/400000 [00:53<00:00, 7497.32it/s] "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CinwDLv5GLJI",
        "colab_type": "text"
      },
      "source": [
        "## Use the text fields below to ask a question on an image\n",
        "\n",
        "Image URL can be any http/https URL. We show top 5 predictions from Pythia. Confidence shows how confident Pythia model was about a particular prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_55MoLFlhL4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import xlwt\n",
        "from xlwt import Workbook\n",
        "from statistics import mode\n",
        "import urllib.request\n",
        "\n",
        "def init_widgets(url, question):\n",
        "  image_text = widgets.Text(\n",
        "    description=\"Image URL\", layout=Layout(minwidth=\"70%\")\n",
        "  )\n",
        "  question_text = widgets.Text(\n",
        "      description=\"Question\", layout=Layout(minwidth=\"70%\")\n",
        "  )\n",
        "\n",
        "  image_text.value = url\n",
        "  question_text.value = question\n",
        "  submit_button = widgets.Button(description=\"Ask Pythia!\")\n",
        "\n",
        "  display(image_text)\n",
        "  display(question_text)\n",
        "  display(submit_button)\n",
        "\n",
        "  submit_button.on_click(lambda b: on_button_click(\n",
        "      b, image_text, question_text\n",
        "  ))\n",
        "  \n",
        "  return image_text, question_text\n",
        "  \n",
        "def on_button_click(b, image_text, question_text):\n",
        "  clear_output()\n",
        "  image_path = demo.get_actual_image(image_text.value)\n",
        "  image = Image.open(image_path)\n",
        "  \n",
        "  scores, predictions = demo.predict(image_text.value, question_text.value)\n",
        "  scores = [score * 100 for score in scores]\n",
        "  df = pd.DataFrame({\n",
        "      \"Prediction\": predictions,\n",
        "      \"Confidence\": scores\n",
        "  })\n",
        "  \n",
        "  init_widgets(image_text.value, question_text.value)\n",
        "  display(image)\n",
        "  \n",
        "  display(HTML(df.to_html()))\n",
        " \n",
        "\n",
        "# image_text, question_text = init_widgets(\n",
        "#    \"https://c8.staticflickr.com/3/2579/5811451782_31f8649055_o.jpg\", \n",
        "#    \"what type of plane is this?\"\n",
        "# )\n",
        "\n",
        " \n",
        "\n",
        "def main():\n",
        "  # ff = open('TextVQA_Rosetta_OCR_v0.2_train.json',)\n",
        "  with urllib.request.urlopen('https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_Rosetta_OCR_v0.2_val.json') as url:\n",
        "      data0 = json.loads(url.read().decode())\n",
        "  ocr_map = {}\n",
        "\n",
        "  j = 1\n",
        "  for i in data0['data']:\n",
        "      answers = \"\"\n",
        "      for c, k in enumerate(i['ocr_tokens']):\n",
        "          if c != 0:\n",
        "              answers += \", \"\n",
        "          answers += k.lower()\n",
        "      ocr_map[i['image_id']] = answers\n",
        "      j += 1\n",
        "\n",
        "\n",
        "  wb = Workbook()\n",
        "  sheet1 = wb.add_sheet('Sheet 1')\n",
        "  # f = open('TextVQA_0.5.1_train.json',)\n",
        "  with urllib.request.urlopen('https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json') as url:\n",
        "      data = json.loads(url.read().decode())\n",
        "  sheet1.write(0, 0, 'Question')\n",
        "  sheet1.write(0, 1, 'Ground Truth')\n",
        "  sheet1.write(0, 2, 'TextVQA Answer')\n",
        "  sheet1.write(0, 3, 'OCR')\n",
        "  sheet1.write(0, 4, 'Ground Truth vs TextVQA')\n",
        "  sheet1.write(0, 5, 'Find if Token exists')\n",
        "  sheet1.write(0, 6, 'Ground Truth Vs  Single OCR')\n",
        "  sheet1.write(0, 7, 'Text VQA Fail OCR Pass')\n",
        "  sheet1.write(0, 8, 'OCR Fail Text VQA Pass')\n",
        "  sheet1.write(0, 9, 'Multiple OCR Combine vs Ground Truth')\n",
        "  sheet1.write(0, 10, 'url')\n",
        "  sheet1.write(0, 11, 'flickr_300k_url')\n",
        "  sheet1.write(0, 12, 'image_id')\n",
        "\n",
        "  j = 1\n",
        "  for i in data['data']:\n",
        "      if j==500:\n",
        "        break\n",
        "      col0 = i['question']\n",
        "      col10 = i['flickr_original_url']\n",
        "      col11 = i['flickr_300k_url']\n",
        "\n",
        "      answers = []\n",
        "      for k in i['answers']:\n",
        "          answers.append(k)\n",
        "      col1 = max(answers, key=answers.count).strip()\n",
        "      if num_there(col1):\n",
        "        continue\n",
        "\n",
        "      col2 = get_pythia_prediction(col0, col10, col11)\n",
        "\n",
        "      col3 = ocr_map[i['image_id']]\n",
        "\n",
        "      if col1.lower() == col2.lower():\n",
        "          col4 = 1\n",
        "      else:\n",
        "        col4 = 0\n",
        "\n",
        "      if col1.lower() in col3:\n",
        "          col5 = 1\n",
        "      else:\n",
        "        col5 = 0\n",
        "\n",
        "      col6 = col5\n",
        "\n",
        "      if col4 == 0 and col6 == 1:\n",
        "          col7 = 1\n",
        "      else:\n",
        "        col7 = 0\n",
        "\n",
        "      if col4 == 1 and col6 == 0:\n",
        "          col8 = 1\n",
        "      else:\n",
        "        col8 = 0\n",
        "\n",
        "      if(col6 == 1 or check_if_multiple_ocr_works(col1.lower(), col3)):\n",
        "          col9 = 1\n",
        "      else:\n",
        "        col9 = 0\n",
        "\n",
        "      sheet1.write(j, 0, col0)\n",
        "      sheet1.write(j, 1, col1)\n",
        "      sheet1.write(j, 2, col2)\n",
        "      sheet1.write(j, 3, col3)\n",
        "      sheet1.write(j, 4, col4)\n",
        "      sheet1.write(j, 5, col5)\n",
        "      sheet1.write(j, 6, col6)\n",
        "      sheet1.write(j, 7, col7)\n",
        "      sheet1.write(j, 8, col8)\n",
        "      sheet1.write(j, 9, col9)\n",
        "\n",
        "      sheet1.write(j, 10, col10)\n",
        "      sheet1.write(j, 11, col11)\n",
        "      sheet1.write(j, 12, i['image_id'])\n",
        "      j+=1\n",
        "\n",
        "  wb.save('ErrorAnalysis4.xls')\n",
        "\n",
        "\n",
        "def check_if_multiple_ocr_works(col1, col3):\n",
        "    answers = []\n",
        "    if \" \" in col1:\n",
        "        answers = col1.split(\" \")\n",
        "        if (check_if_all_answer_parts_exist(answers, col3)):\n",
        "            return True\n",
        "    if \",\" in col1:\n",
        "        answers = col1.split(\",\")\n",
        "        if (check_if_all_answer_parts_exist(answers, col3)):\n",
        "            return True\n",
        "    if \".\" in col1:\n",
        "        answers = col1.split(\".\")\n",
        "        if (check_if_all_answer_parts_exist(answers, col3)):\n",
        "            return True\n",
        "    if \"-\" in col1:\n",
        "        answers = col1.split(\"-\")\n",
        "        if (check_if_all_answer_parts_exist(answers, col3)):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def check_if_all_answer_parts_exist(answers, col3):\n",
        "    for ans in answers:\n",
        "        if ans not in col3:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_pythia_prediction(col0, col10, col11):\n",
        "    try:\n",
        "        scores, predictions = demo.predict(col10, col0)\n",
        "        scores = [score * 100 for score in scores]\n",
        "        return predictions[0]\n",
        "\n",
        "    except:\n",
        "        return try_300k_image_url(col0, col11)\n",
        "\n",
        "\n",
        "def try_300k_image_url(col0, col11):\n",
        "    try:\n",
        "        scores, predictions = demo.predict(col11, col0)\n",
        "        scores = [score * 100 for score in scores]\n",
        "        return predictions[0]\n",
        "\n",
        "    except:\n",
        "        return 'Exception'\n",
        "\n",
        "def num_there(s):\n",
        "    return any(i.isdigit() for i in s)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}